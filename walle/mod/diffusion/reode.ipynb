{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1- how to insert images https://mljar.com/blog/jupyter-notebook-insert-image/\n",
    "2- autodownload paper in chapters divided by cells\n",
    "3- mathpix for autowriting equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jonathon Ho, Google, 2022](https://arxiv.org/pdf/2204.03458.pdf)\n",
    "\n",
    "Generating temporally coherent high fidelity video is an important milestone in\n",
    "generative modeling research. We make progress towards this milestone by proposing a diffusion model for video generation that shows very promising initial results.\n",
    "Our model is a natural extension of the standard image diffusion architecture, and\n",
    "it enables jointly training from image and video data, which we find to reduce the\n",
    "variance of minibatch gradients and speed up optimization. To generate long and\n",
    "higher resolution videos we introduce a new conditional sampling technique for\n",
    "spatial and temporal video extension that performs better than previously proposed\n",
    "methods. We present the first results on a large text-conditioned video generation\n",
    "task, as well as state-of-the-art results on established benchmarks for video prediction and unconditional video generation. Supplementary material is available at\n",
    "\n",
    "[code](https://video-diffusion.github.io/)\n",
    "\n",
    "q(zt|x) = N (zt; αtx, σ2\n",
    "t\n",
    "I), q(zt|zs) = N (zt; (αt/αs)zs, σ2\n",
    "t|s\n",
    "I) \n",
    "\n",
    "image.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url, file_name, headers):\n",
    "    import requests    # Send GET request\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # Save the PDF\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "\n",
    "# Define HTTP Headers\n",
    "headers = {\n",
    "    \"User-Agent\": \"Chrome/51.0.2704.103\",\n",
    "}\n",
    "# Define URL of an image\n",
    "url = 'https://arxiv.org/pdf/2204.03458.pdf'\n",
    "# Define image file name\n",
    "paper = \"hoGoogleDiffVid.pdf\"\n",
    "# Download image\n",
    "download_pdf(url, paper, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 10:49:08,237 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to C:\\Users\\max\\AppData\\Local\\Temp\\tika-server.jar.\n",
      "2022-11-10 10:49:14,897 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to C:\\Users\\max\\AppData\\Local\\Temp\\tika-server.jar.md5.\n",
      "2022-11-10 10:49:15,384 [MainThread  ] [ERROR]  Unable to run java; is it installed?\n",
      "2022-11-10 10:49:15,386 [MainThread  ] [ERROR]  Failed to receive startup confirmation from startServer.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to start Tika server.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\max\\OneDrive\\sisy\\walle\\walle\\mod\\diffusion\\reode.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/max/OneDrive/sisy/walle/walle/mod/diffusion/reode.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtika\u001b[39;00m \u001b[39mimport\u001b[39;00m parser \u001b[39m# pip install tika\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/max/OneDrive/sisy/walle/walle/mod/diffusion/reode.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m raw \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39;49mfrom_file(paper)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/max/OneDrive/sisy/walle/walle/mod/diffusion/reode.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(raw[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\max\\miniconda3\\envs\\hotseat\\lib\\site-packages\\tika\\parser.py:40\u001b[0m, in \u001b[0;36mfrom_file\u001b[1;34m(filename, serverEndpoint, service, xmlContent, headers, config_path, requestOptions)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mParses a file for metadata and content\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m:param filename: path to file which needs to be parsed or binary file using open(path,'rb')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39m        'content' has a str value and metadata has a dict type value.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m xmlContent:\n\u001b[1;32m---> 40\u001b[0m     output \u001b[39m=\u001b[39m parse1(service, filename, serverEndpoint, headers\u001b[39m=\u001b[39;49mheaders, config_path\u001b[39m=\u001b[39;49mconfig_path, requestOptions\u001b[39m=\u001b[39;49mrequestOptions)\n\u001b[0;32m     41\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     output \u001b[39m=\u001b[39m parse1(service, filename, serverEndpoint, services\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/meta\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/tika\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/rmeta/xml\u001b[39m\u001b[39m'\u001b[39m},\n\u001b[0;32m     43\u001b[0m                         headers\u001b[39m=\u001b[39mheaders, config_path\u001b[39m=\u001b[39mconfig_path, requestOptions\u001b[39m=\u001b[39mrequestOptions)\n",
      "File \u001b[1;32mc:\\Users\\max\\miniconda3\\envs\\hotseat\\lib\\site-packages\\tika\\tika.py:336\u001b[0m, in \u001b[0;36mparse1\u001b[1;34m(option, urlOrPath, serverEndpoint, verbose, tikaServerJar, responseMimeType, services, rawResponse, headers, config_path, requestOptions)\u001b[0m\n\u001b[0;32m    334\u001b[0m headers\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mAccept\u001b[39m\u001b[39m'\u001b[39m: responseMimeType, \u001b[39m'\u001b[39m\u001b[39mContent-Disposition\u001b[39m\u001b[39m'\u001b[39m: make_content_disposition_header(path\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39mis\u001b[39;00m unicode_string \u001b[39melse\u001b[39;00m path)})\n\u001b[0;32m    335\u001b[0m \u001b[39mwith\u001b[39;00m urlOrPath \u001b[39mif\u001b[39;00m _is_file_object(urlOrPath) \u001b[39melse\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 336\u001b[0m     status, response \u001b[39m=\u001b[39m callServer(\u001b[39m'\u001b[39;49m\u001b[39mput\u001b[39;49m\u001b[39m'\u001b[39;49m, serverEndpoint, service, f,\n\u001b[0;32m    337\u001b[0m                                   headers, verbose, tikaServerJar, config_path\u001b[39m=\u001b[39;49mconfig_path,\n\u001b[0;32m    338\u001b[0m                                   rawResponse\u001b[39m=\u001b[39;49mrawResponse, requestOptions\u001b[39m=\u001b[39;49mrequestOptions)\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m file_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mremote\u001b[39m\u001b[39m'\u001b[39m: os\u001b[39m.\u001b[39munlink(path)\n\u001b[0;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m (status, response)\n",
      "File \u001b[1;32mc:\\Users\\max\\miniconda3\\envs\\hotseat\\lib\\site-packages\\tika\\tika.py:531\u001b[0m, in \u001b[0;36mcallServer\u001b[1;34m(verb, serverEndpoint, service, data, headers, verbose, tikaServerJar, httpVerbs, classpath, rawResponse, config_path, requestOptions)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[39mglobal\u001b[39;00m TikaClientOnly\n\u001b[0;32m    530\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m TikaClientOnly:\n\u001b[1;32m--> 531\u001b[0m     serverEndpoint \u001b[39m=\u001b[39m checkTikaServer(scheme, serverHost, port, tikaServerJar, classpath, config_path)\n\u001b[0;32m    533\u001b[0m serviceUrl  \u001b[39m=\u001b[39m serverEndpoint \u001b[39m+\u001b[39m service\n\u001b[0;32m    534\u001b[0m \u001b[39mif\u001b[39;00m verb \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m httpVerbs:\n",
      "File \u001b[1;32mc:\\Users\\max\\miniconda3\\envs\\hotseat\\lib\\site-packages\\tika\\tika.py:601\u001b[0m, in \u001b[0;36mcheckTikaServer\u001b[1;34m(scheme, serverHost, port, tikaServerJar, classpath, config_path)\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m status:\n\u001b[0;32m    600\u001b[0m             log\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mFailed to receive startup confirmation from startServer.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 601\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to start Tika server.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    602\u001b[0m \u001b[39mreturn\u001b[39;00m serverEndpoint\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to start Tika server."
     ]
    }
   ],
   "source": [
    "from tika import parser # pip install tika\n",
    "\n",
    "raw = parser.from_file(paper)\n",
    "print(raw['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader(paper)\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text() + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Diffusion Models\n",
      "Jonathan Ho\u0003\n",
      "jonathanho@google.comTim Salimans\u0003\n",
      "salimans@google.comAlexey Gritsenko\n",
      "agritsenko@google.com\n",
      "William Chan\n",
      "williamchan@google.comMohammad Norouzi\n",
      "mnorouzi@google.comDavid J. Fleet\n",
      "davidfleet@google.com\n",
      "Abstract\n",
      "Generating temporally coherent high ﬁdelity video is an important milestone in\n",
      "generative modeling research. We make progress towards this milestone by propos-\n",
      "ing a diffusion model for video generation that shows very promising initial results.\n",
      "Our model is a natural extension of the standard image diffusion architecture, and\n",
      "it enables jointly training from image and video data, which we ﬁnd to reduce the\n",
      "variance of minibatch gradients and speed up optimization. To generate long and\n",
      "higher resolution videos we introduce a new conditional sampling technique for\n",
      "spatial and temporal video extension that performs better than previously proposed\n",
      "methods. We present the ﬁrst results on a large text-conditioned video generation\n",
      "task, as well as state-of-the-art results on established benchmarks for video predic-\n",
      "tion and unconditional video generation. Supplementary material is available at\n",
      "https://video-diffusion.github.io/ .\n",
      "1 Introduction\n",
      "Diffusion models have recently been producing high quality results in image generation and audio\n",
      "generation [e.g. 28,39,40,16,23,36,48,60,42,10,29], and there is signiﬁcant interest in validating\n",
      "diffusion models in new data modalities. In this work, we present ﬁrst results on video generation\n",
      "using diffusion models, for both unconditional and conditional settings.\n",
      "We show that high quality videos can be generated using essentially the standard formulation of\n",
      "the Gaussian diffusion model [ 46], with little modiﬁcation other than straightforward architectural\n",
      "changes to accommodate video data within the memory constraints of deep learning accelerators.\n",
      "We train models that generate a ﬁxed number of video frames using a 3D U-Net diffusion model\n",
      "architecture, and we enable generating longer videos by applying this model autoregressively using a\n",
      "new method for conditional generation. We additionally show the beneﬁts of joint training on video\n",
      "and image modeling objectives. We test our methods on video prediction and unconditional video\n",
      "generation, where we achieve state-of-the-art sample quality scores, and we also show promising ﬁrst\n",
      "results on text-conditioned video generation.\n",
      "2 Background\n",
      "A diffusion model [ 46,47,22] speciﬁed in continuous time [ 53,48,10,28] is a generative model\n",
      "with latents z=fztjt2[0;1]gobeying a forward process q(zjx)starting at data x\u0018p(x). The\n",
      "forward process is a Gaussian process that satisﬁes the Markovian structure:\n",
      "q(ztjx) =N(zt;\u000btx;\u001b2\n",
      "tI); q(ztjzs) =N(zt; (\u000bt=\u000bs)zs;\u001b2\n",
      "tjsI) (1)\n",
      "where 0\u0014s < t\u00141,\u001b2\n",
      "tjs= (1\u0000e\u0015t\u0000\u0015s)\u001b2\n",
      "t, and\u000bt;\u001btspecify a differentiable noise schedule\n",
      "whose log signal-to-noise-ratio \u0015t= log[\u000b2\n",
      "t=\u001b2\n",
      "t]decreases with tuntilq(z1)\u0019N(0;I).\n",
      "\u0003Equal contributionarXiv:2204.03458v2  [cs.CV]  22 Jun 2022\n",
      "Training Learning to reverse the forward process for generation can be reduced to learning to\n",
      "denoise zt\u0018q(ztjx)into an estimate ^x\u0012(zt;\u0015t)\u0019xfor allt(we will drop the dependence on \u0015tto\n",
      "simplify notation). We train this denoising model ^x\u0012using a weighted mean squared error loss\n",
      "E\u000f;t\u0002\n",
      "w(\u0015t)k^x\u0012(zt)\u0000xk2\n",
      "2\u0003\n",
      "(2)\n",
      "over uniformly sampled times t2[0;1]. This reduction of generation to denoising can be justiﬁed\n",
      "as optimizing a weighted variational lower bound on the data log likelihood under the diffusion\n",
      "model, or as a form of denoising score matching [ 56,47,22,28]. In practice, we use the \u000f-prediction\n",
      "parameterization, deﬁned as ^x\u0012(zt) = (zt\u0000\u001bt\u000f\u0012(zt))=\u000bt, and train \u000f\u0012using a mean squared error\n",
      "in\u000fspace withtsampled according to a cosine schedule [ 37]. This corresponds to a particular\n",
      "weightingw(\u0015t)for learning a scaled score estimate \u000f\u0012(zt)\u0019\u0000\u001btrztlogp(zt), wherep(zt)is the\n",
      "true density of ztunder x\u0018p(x)[22,28,48]. We also train using the v-prediction parameterization\n",
      "for certain models [42].\n",
      "Sampling We use a variety of diffusion model samplers in this work. One is the discrete time\n",
      "ancestral sampler [ 22] with sampling variances derived from lower and upper bounds on reverse\n",
      "process entropy [ 46,22,37]. To deﬁne this sampler, ﬁrst note that the forward process can be\n",
      "described in reverse as q(zsjzt;x) =N(zs;~\u0016sjt(zt;x);~\u001b2\n",
      "sjtI)(notings<t ), where\n",
      "~\u0016sjt(zt;x) =e\u0015t\u0000\u0015s(\u000bs=\u000bt)zt+ (1\u0000e\u0015t\u0000\u0015s)\u000bsxand ~\u001b2\n",
      "sjt= (1\u0000e\u0015t\u0000\u0015s)\u001b2\n",
      "s: (3)\n",
      "Starting at z1\u0018N(0;I), the ancestral sampler follows the rule\n",
      "zs=~\u0016sjt(zt;^x\u0012(zt)) +q\n",
      "(~\u001b2\n",
      "(\u001b2)1\u0000\n",
      "\u000f (4)\n",
      "is a hyperparameter that controls the stochasticity of the\n",
      "sampler [37], and s;tfollow a uniformly spaced sequence from 1 to 0.\n",
      "Another sampler, which we found especially effective with our new method for conditional gener-\n",
      "ation (Section 3.1), is the predictor-corrector sampler [ 48]. Our version of this sampler alternates\n",
      "between the ancestral sampler step (4) and a Langevin correction step of the form\n",
      "zs zs\u00001\n",
      "2\u000e\u001bs\u000f\u0012(zs) +p\n",
      "\u000e\u001bs\u000f0(5)\n",
      "where\u000eis a step size which we ﬁx to 0:1here, and \u000f0is another independent sample of standard\n",
      "Gaussian noise. The purpose of the Langevin step is to help the marginal distribution of each zs\n",
      "generated by the sampler to match the true marginal under the forward process starting at x\u0018p(x).\n",
      "In the conditional generation setting, the data xis equipped with a conditioning signal c, which\n",
      "may represent a class label, text caption, or other type of conditioning. To train a diffusion model\n",
      "to ﬁtp(xjc), the only modiﬁcation that needs to be made is to provide cto the model as ^x\u0012(zt;c).\n",
      "Improvements to sample quality can be obtained in this setting by using classiﬁer-free guidance [20].\n",
      "This method samples using adjusted model predictions ~\u000f\u0012, constructed via\n",
      "~\u000f\u0012(zt;c) = (1 +w)\u000f\u0012(zt;c)\u0000w\u000f\u0012(zt); (6)\n",
      "wherewis the guidance strength ,\u000f\u0012(zt;c) =1\n",
      "\u001bt(zt\u0000^x\u0012(zt;c))is the regular conditional model\n",
      "prediction, and \u000f\u0012(zt)is a prediction from an unconditional model jointly trained with the conditional\n",
      "model (if cconsists of embedding vectors, unconditional modeling can be represented as c=0). For\n",
      "w> 0this adjustment has the effect of over-emphasizing the effect of conditioning on the signal c,\n",
      "which tends to produce samples of lower diversity but higher quality compared to sampling from\n",
      "the regular conditional model [ 20]. The method can be interpreted as a way to guide the samples\n",
      "towards areas where an implicit classiﬁer p(cjzt)has high likelihood, and is an adaptation of the\n",
      "explicit classiﬁer guidance method proposed by [16].\n",
      "3 Video diffusion models\n",
      "Our approach to video generation using diffusion models is to use the standard diffusion model\n",
      "formalism described in Section 2 with a neural network architecture suitable for video data. Each\n",
      "of our models is trained to jointly model a ﬁxed number of frames at a ﬁxed spatial resolution. To\n",
      "2\n",
      "extend sampling to longer sequences of frames or higher spatial resolutions, we will repurpose our\n",
      "models with a conditioning technique described later in Section 3.1.\n",
      "In prior work on image modeling, the standard architecture for ^x\u0012in an image diffusion model is a\n",
      "U-Net [ 38,44], which is a neural network architecture constructed as a spatial downsampling pass\n",
      "followed by a spatial upsampling pass with skip connections to the downsampling pass activations.\n",
      "The network is built from layers of 2D convolutional residual blocks, for example in the style\n",
      "of the Wide ResNet [ 65], and each such convolutional block is followed by a spatial attention\n",
      "block [ 55,58,11]. Conditioning information, such as cand\u0015t, is provided to the network in the\n",
      "form of an embedding vector added into each residual block (we ﬁnd it helpful for our models to\n",
      "process these embedding vectors using several MLP layers before adding).\n",
      "We propose to extend this image diffusion model architecture to video data, given by a block of\n",
      "a ﬁxed number of frames, using a particular type of 3D U-Net [ 13] that is factorized over space\n",
      "and time. First, we modify the image model architecture by changing each 2D convolution into a\n",
      "space-only 3D convolution, for instance, we change each 3x3 convolution into a 1x3x3 convolution\n",
      "(the ﬁrst axis indexes video frames, the second and third index the spatial height and width). The\n",
      "attention in each spatial attention block remains as attention over space; i.e., the ﬁrst axis is treated\n",
      "as a batch axis. Second, after each spatial attention block, we insert a temporal attention block that\n",
      "performs attention over the ﬁrst axis and treats the spatial axes as batch axes. We use relative position\n",
      "embeddings [ 45] in each temporal attention block so that the network can distinguish ordering of\n",
      "frames in a way that does not require an absolute notion of video time. We visualize the model\n",
      "architecture in Fig. 1.\n",
      "(zt;c;\u0015t)N2,M1(N\n",
      "2)2,M2(N\n",
      "K)2,MK (N\n",
      "K)2,MK (N\n",
      "K)2,2\u0002MK\n",
      "(N\n",
      "2)2,2\u0002M2\n",
      "N2,2\u0002M1 ^x\n",
      "Figure 1: The 3D U-Net architecture for ^x\u0012in the diffusion model. Each block represents a 4D\n",
      "tensor with axes labeled as frames\u0002height\u0002width\u0002channels , processed in a space-time factorized\n",
      "manner as described in Section 3. The input is a noisy video zt, conditioning c, and the log SNR \u0015t.\n",
      "The downsampling/upsampling blocks adjust the spatial input resolution height\u0002width by a factor\n",
      "of 2 through each of the Kblocks. The channel counts are speciﬁed using channel multipliers M1,\n",
      "M2, ...,MK, and the upsampling pass has concatenation skip connections to the downsampling pass.\n",
      "The use of factorized space-time attention is known to be a good choice in video transformers for its\n",
      "computational efﬁciency [ 2,5,21]. An advantage of our factorized space-time architecture, which is\n",
      "unique to our video generation setting, is that it is particularly straightforward to mask the model to\n",
      "run on independent images rather than a video, simply by removing the attention operation inside\n",
      "each time attention block and ﬁxing the attention matrix to exactly match each key and query vector\n",
      "at each video timestep. The utility of doing so is that it allows us to jointly train the model on both\n",
      "video and image generation. We ﬁnd in our experiments that this joint training is important for sample\n",
      "quality (Section 4).\n",
      "3.1 Reconstruction-guided sampling for improved conditional generation\n",
      "The videos we consider modeling typically consist of hundreds to thousands of frames, at a frame rate\n",
      "of at least 24 frames per second. To manage the computational requirements of training our models,\n",
      "we only train on a small subset of say 16 frames at a time. However, at test time we can generate\n",
      "longer videos by extending our samples. For example, we could ﬁrst generate a video xa\u0018p\u0012(x)\n",
      "3\n",
      "consisting of 16 frames, and then extend it with a second sample xb\u0018p\u0012(xbjxa). Ifxbconsists of\n",
      "frames following xa, this allows us to autoregressively extend our sampled videos to arbitrary lengths,\n",
      "which we demonstrate in Section 4.3.3. Alternatively, we could choose xato represent a video of\n",
      "lower frame rate, and then deﬁne xbto be those frames in between the frames of xa. This allows one\n",
      "to then to upsample a video temporally, similar to how [ 34] generate high resolution images through\n",
      "spatial upsampling.\n",
      "Both approaches require one to sample from a conditional model, p\u0012(xbjxa). This conditional model\n",
      "could be trained explicitly, but it can also be derived approximately from our unconditional model\n",
      "p\u0012(x)by imputation, which has the advantage of not requiring a separately trained model. For\n",
      "example, [ 48] present a general method for conditional sampling from a jointly trained diffusion\n",
      "modelp\u0012(x= [xa;xb]): In their approach to sampling from p\u0012(xbjxa), the sampling procedure for\n",
      "updating zb\n",
      "sis unchanged from the standard method for sampling from p\u0012(zsjzt), with zs= [za\n",
      "s;zb\n",
      "s],\n",
      "but the samples for za\n",
      "sare replaced by exact samples from the forward process, q(za\n",
      "sjxa), at each\n",
      "iteration. The samples za\n",
      "sthen have the correct marginal distribution by construction, and the samples\n",
      "zb\n",
      "swill conform with za\n",
      "sthrough their effect on the denoising model ^x\u0012([za\n",
      "t;zb\n",
      "t]). Similarly, we could\n",
      "sample za\n",
      "sfromq(za\n",
      "sjxa;za\n",
      "t), which follows the correct conditional distribution in addition to the\n",
      "correct marginal. We will refer to both of these approaches as the replacement method for conditional\n",
      "sampling from diffusion models.\n",
      "When we tried the replacement method to conditional sampling, we found it to not work well for\n",
      "our video models: Although samples xblooked good in isolation, they were often not coherent\n",
      "withxa. This is caused by a fundamental problem with this replacement sampling method. That\n",
      "is, the latents zb\n",
      "sare updated in the direction provided by ^xb\n",
      "\u0012(zt)\u0019Eq[xbjzt], while what is\n",
      "needed instead is Eq[xbjzt;xa]. Writing this in terms of the score of the data distribution, we get\n",
      "Eq[xbjzt;xa] =Eq[xbjzt] + (\u001b2\n",
      "t=\u000bt)rzb\n",
      "tlogq(xajzt), where the second term is missing in the\n",
      "replacement method. Assuming a perfect denoising model, plugging in this missing term would\n",
      "make conditional sampling exact. Since q(xajzt)is not available in closed form, however, we instead\n",
      "propose to approximate it using a Gaussian of the form q(xajzt)\u0019N [^xa\n",
      "\u0012(zt);(\u001b2\n",
      "t=\u000b2\n",
      "t)I], where\n",
      "^xa\n",
      "\u0012(zt)is a reconstruction of the conditioning data xaprovided by our denoising model. Assuming a\n",
      "perfect model, this approximation becomes exact as t!0, and empirically we ﬁnd it to be good for\n",
      "largertalso. Plugging in the approximation, and adding a weighting factor wr, our proposed method\n",
      "to conditional sampling is a variant of the replacement method with an adjusted denoising model, ~xb\n",
      "\u0012,\n",
      "deﬁned by\n",
      "~xb\n",
      "\u0012(zt) =^xb\n",
      "\u0012(zt)\u0000wr\u000bt\n",
      "2rzb\n",
      "tkxa\u0000^xa\n",
      "\u0012(zt)k2\n",
      "2: (7)\n",
      "The additional gradient term in this expression can be interpreted as a form of guidance [16,20]\n",
      "based on the model’s reconstruction of the conditioning data, and we therefore refer to this method\n",
      "asreconstruction-guided sampling , or simply reconstruction guidance . Like with other forms\n",
      "of guidance, we ﬁnd that choosing a larger weighting factor, wr>1, tends to improve sample\n",
      "quality. We empirically investigate reconstruction guidance in Section 4.3.3, where we ﬁnd it to\n",
      "work surprisingly well, especially when combined with predictor-corrector samplers using Langevin\n",
      "diffusion [48].\n",
      "Reconstruction guidance also extends to the case of spatial interpolation (or super-resolution), in\n",
      "which the mean squared error loss is imposed on a downsampled version of the model prediction,\n",
      "and backpropagation is performed through this downsampling. In this setting, we have low resolution\n",
      "ground truth videos xa(e.g. at the 64x64 spatial resolution), which may be generated from a low\n",
      "resolution model, and we wish to upsample them into high resolution videos (e.g. at the 128x128\n",
      "spatial resolution) using an unconditional high resolution diffusion model ^x\u0012. To accomplish this, we\n",
      "adjust the high resolution model as follows:\n",
      "~x\u0012(zt) =^x\u0012(zt)\u0000wr\u000bt\n",
      "2rztkxa\u0000^xa\n",
      "\u0012(zt)k2\n",
      "2 (8)\n",
      "where ^xa\n",
      "\u0012(zt)is our model’s reconstruction of the low-resolution video from zt, which is obtained\n",
      "by downsampling the high resolution output of the model using a differentiable downsampling\n",
      "algorithm such as bilinear interpolation. Note that it is also possible to simultaneously condition on\n",
      "low resolution videos while autoregressively extending samples at the high resolution using the same\n",
      "reconstruction guidance method. In Fig. 2, we show samples of this approach for extending 16x64x64\n",
      "low resolution samples at frameskip 4 to 64x128x128 samples at frameskip 1 using a 9x128x128\n",
      "diffusion model.\n",
      "4\n",
      "4 Experiments\n",
      "We report our results on video diffusion models for unconditional video generation (Section 4.1),\n",
      "conditional video generation (video prediction) (Section 4.2), and text-conditioned video genera-\n",
      "tion (Section 4.3). We evaluate our models using standard metrics such as FVD [ 54], FID [ 19], and\n",
      "IS [43]; details on evaluation are provided below alongside each benchmark. Samples and additional\n",
      "results are provided at https://video-diffusion.github.io/ . Architecture hyperparameters,\n",
      "training details, and compute resources are listed in Appendix A.\n",
      "4.1 Unconditional video modeling\n",
      "To demonstrate our approach on unconditional generation, we use a popular benchmark of Soomro\n",
      "et al. [49] for unconditional modeling of video. The benchmark consists of short clips of people\n",
      "performing one of 101 activities, and was originally collected for the purpose of training action\n",
      "recognition models. We model short segments of 16 frames from this dataset, downsampled to a\n",
      "spatial resolution of 64x64. In Table 1 we present perceptual quality scores for videos generated by\n",
      "our model, and we compare against methods from the literature, ﬁnding that our method strongly\n",
      "improves upon the previous state-of-the-art.\n",
      "We use the data loader provided by TensorFlow Datasets [ 1] without further processing, and we train\n",
      "on all 13,320 videos. Similar to previous methods, we use the C3D network [ 51]2for calculating\n",
      "FID and IS, using 10,000 samples generated from our model. C3D internally resizes input data to\n",
      "the 112x112 spatial resolution, so perceptual scores are approximately comparable even when the\n",
      "data is sampled at a different resolution originally. As discussed by [ 64], methods in the literature\n",
      "are unfortunately not always consistent in the data preprocessing that is used, which may lead to\n",
      "small differences in reported scores between papers. The Inception Score we calculate for real data\n",
      "(\u001960) is consistent with that reported by [26], who also report a higher real data Inception score of\n",
      "\u001990for data sampled at the 128x128 resolution, which indicates that our 64x64 model might be at a\n",
      "disadvantage compared to works that generate at a higher resolution. Nevertheless, our model obtains\n",
      "the best perceptual quality metrics that we could ﬁnd in the literature.\n",
      "Method Resolution FID # IS\"\n",
      "MoCoGAN [52] 16x64x64 26998 \u000633 12.42\n",
      "TGAN-F [26] 16x64x64 8942.63 \u00063.72 13.62\n",
      "TGAN-ODE [18] 16x64x64 26512 \u000627 15.2\n",
      "TGAN-F [26] 16x128x128 7817 \u000610 22.91\u0006.19\n",
      "VideoGPT [62] 16x128x128 24.69 \u00060.30\n",
      "TGAN-v2 [41] 16x64x64 3431 \u000619 26.60\u00060.47\n",
      "TGAN-v2 [41] 16x128x128 3497 \u000626 28.87\u00060.47\n",
      "DVD-GAN [14] 16x128x128 32.97 \u00061.7\n",
      "Video Diffusion (ours) 16x64x64 295\u00063 57\u00060.62\n",
      "real data 16x64x64 60.2\n",
      "Table 1: Unconditional video modeling results on UCF101.\n",
      "4.2 Video prediction\n",
      "A common benchmark task for evaluating generative models of video is video prediction , where the\n",
      "model is given the ﬁrst frame(s) of a video and is asked to generate the remainder. Models that do\n",
      "well on this conditional generation task are usually trained explicitly for this conditional setting,\n",
      "for example by being autoregressive across frames. Although our models are instead only trained\n",
      "unconditionally, we can adapt them to the video prediction setting by using the guidance method\n",
      "proposed in section 3.1. Here we evaluate this method on two popular video prediction benchmarks,\n",
      "obtaining state-of-the-art results.\n",
      "BAIR Robot Pushing We evaluate video prediction performance on BAIR Robot Pushing [ 17], a\n",
      "standard benchmark in the video literature consisting of approximately 44000 videos of robot pushing\n",
      "2We use the C3D model as implemented at github.com/pfnet-research/tgan2 [41].\n",
      "5\n",
      "motions at the 64x64 spatial resolution. Methods for this benchmark are conditioned on 1 frame\n",
      "and generate the next 15. Results are listed in Table 2. Following the evaluation protocol of [ 4] and\n",
      "others, we calculate FVD [ 54] using the I3D network [ 8] by comparing 100\u0002256model samples\n",
      "against the 256examples in the evaluation set.\n",
      "Kinetics-600 We additionally evaluate video prediction performance on the Kinetics-600 bench-\n",
      "mark [ 27,9]. Kinetics-600 contains approximately 400 thousand training videos depicting 600\n",
      "different activities. We train unconditional models on this dataset at the 64\u000264resolution and evalu-\n",
      "ate on 50 thousand randomly sampled videos from the test set, where we condition on a randomly\n",
      "sampled subsequence of 5 frames and generate the next 11 frames. Like previous works, we calculate\n",
      "FVD and Inception Score using the I3D network [ 8]. See Table 3 for results. In our reported results\n",
      "we sample test videos without replacement, and we use the same randomly selected subsequences\n",
      "for generating model samples and for deﬁning the ground truth, since this results in the lowest bias\n",
      "and variance in the reported FVD metric. However, from personal communication we learned that\n",
      "[33,14] instead sampled with replacement , and used a different random seed when sampling the\n",
      "ground truth data. We ﬁnd that this way of evaluating raises the FVD obtained by our model slightly,\n",
      "from 16:2to16:9. Inception Score is unaffected.\n",
      "Table 2: Video prediction on BAIR Robot Pushing.\n",
      "Method FVD #\n",
      "DVD-GAN [14] 109.8\n",
      "VideoGPT [62] 103.3\n",
      "TrIVD-GAN-FP [33] 103.3\n",
      "Transframer [35] 100\n",
      "CCVS [31] 99\n",
      "VideoTransformer [59] 94\n",
      "FitVid [4] 93.6\n",
      "NUWA [61] 86.9\n",
      "Video Diffusion (ours)\n",
      "ancestral sampler, 512 steps 68.19\n",
      "Langevin sampler, 256 steps 66.92Table 3: Video prediction on Kinetics-600.\n",
      "Method FVD # IS\"\n",
      "Video Transformer [59] 170 \u00065\n",
      "DVD-GAN-FP [14] 69.1 \u00060.78\n",
      "Video VQ-V AE [57] 64.3 \u00062.04\n",
      "CCVS [31] 55 \u00061\n",
      "TrIVD-GAN-FP [33] 25.74 \u00060.66 12.54\n",
      "Transframer [35] 25.4\n",
      "Video Diffusion (ours)\n",
      "ancestral, 256 steps 18.6 15.39\n",
      "Langevin, 128 steps 16.2\u00060.34 15.64\n",
      "4.3 Text-conditioned video generation\n",
      "The remaining experiments reported are on text-conditioned video generation. In this text-conditioned\n",
      "video generation setting, we employ a dataset of 10 million captioned videos, and we condition the\n",
      "diffusion model on captions in the form of BERT-large embeddings [ 15] processed using attention\n",
      "pooling. We consider two model sizes: a small model for the joint training ablation, and a large model\n",
      "for generating the remaining results (both architectures are described in detail in Appendix A), and\n",
      "we explore the effects of joint video-image training, classiﬁer-free guidance, and our newly proposed\n",
      "reconstruction guidance method for autoregressive extension and simultaneous spatial and temporal\n",
      "super-resolution. We report the following metrics in this section on 4096 samples: the video metric\n",
      "FVD, and the Inception-based image metrics FID and IS measured by averaging activations across\n",
      "frames (FID/IS-avg) and by measuring the ﬁrst frame only (FID/IS-ﬁrst). For FID and FVD, we\n",
      "report two numbers which are measured against the training and validation sets, respectively. For IS,\n",
      "we report two numbers which are averaged scores across 1 split and 10 splits of samples, respectively.\n",
      "4.3.1 Joint training on video and image modeling\n",
      "As described in Section 3, one of the main advantages of our video architecture is that it allows us\n",
      "to easily train the model jointly on video and image generative modeling objectives. To implement\n",
      "this joint training, we concatenate random independent image frames to the end of each video\n",
      "sampled from the dataset, and we mask the attention in the temporal attention blocks to prevent\n",
      "mixing information across video frames and each individual image frame. We choose these random\n",
      "independent images from random videos within the same dataset; in future work we plan to explore\n",
      "the effect of choosing images from other larger image-only datasets.\n",
      "6\n",
      "Figure 2: Text-conditioned video samples from a cascade of two models. First samples are generated\n",
      "from a 16x64x64 frameskip 4 model. Then those samples are treated as ground truth for simultaneous\n",
      "super-resolution and autoregressive extension to 64x128x128 using a 9x128x128 frameskip 1 model.\n",
      "Both models are conditioned on the text prompt. In this ﬁgure, the text prompt, low resolution frames,\n",
      "and high resolution frames are visualized in sequence. See Fig. 5 for more samples.\n",
      "Table 4 reports results for an experiment on text-conditioned 16x64x64 videos, where we consider\n",
      "training on an additional 0, 4, or 8 independent image frames per video. One can see clear improve-\n",
      "ments in video and image sample quality metrics as more independent image frames are added.\n",
      "Adding independent image frames has the effect of reducing variance of the gradient at the expense\n",
      "of some bias for the video modeling objective, and thus it can be seen as a memory optimization to ﬁt\n",
      "more independent examples in a batch.\n",
      "Table 4: Improved sample quality due to image-video joint training on text-to-video generation.\n",
      "Image frames FVD # FID-avg# IS-avg\" FID-ﬁrst# IS-ﬁrst\"\n",
      "0 202.28/205.42 37.52/37.40 7.91/7.58 41.14/40.87 9.23/8.74\n",
      "4 68.11/70.74 18.62/18.42 9.02/8.53 22.54/22.19 10.58/9.91\n",
      "8 57.84/60.72 15.57/15.44 9.32/8.82 19.25/18.98 10.81/10.12\n",
      "4.3.2 Effect of classiﬁer-free guidance\n",
      "Table 5 reports results that verify the effectiveness of classiﬁer-free guidance [ 20] on text-to-video\n",
      "generation. As expected, there is clear improvement in the Inception Score-like metrics with higher\n",
      "guidance weight, while the FID-like metrics improve and then degrade with increasing guidance\n",
      "weight. Similar ﬁndings have been reported on text-to-image generation [36].\n",
      "Figure 3 shows the effect of classiﬁer-free guidance [ 20] on a text-conditioned video model. Similar\n",
      "to what was observed in other work that used classiﬁer-free guidance on text-conditioned image\n",
      "generation [ 36] and class-conditioned image generation [ 20,16], adding guidance increases the\n",
      "sample ﬁdelity of each individual image and emphases the effect of the conditioning signal.\n",
      "7\n",
      "Figure 3: Example frames from a random selection of videos generated by our 16x64x64 text-\n",
      "conditioned model. Left: unguided samples, right: guided samples using classiﬁer-free guidance.\n",
      "Table 5: Effect of classiﬁer-free guidance on text-to-video generation (large models). Sample quality\n",
      "is reported for 16x64x64 models trained on frameskip 1 and 4 data. The model was jointly trained on\n",
      "8 independent image frames per 16-frame video.\n",
      "Frameskip Guidance weight FVD # FID-avg# IS-avg\" FID-ﬁrst# IS-ﬁrst\"\n",
      "1 1.0 41.65/43.70 12.49/12.39 10.80/10.07 16.42/16.19 12.17/11.22\n",
      "2.0 50.19/48.79 10.53/10.47 13.22/12.10 13.91/13.75 14.81/13.46\n",
      "5.0 163.74/160.21 13.54/13.52 14.80/13.46 17.07/16.95 16.40/14.75\n",
      "4 1.0 56.71/60.30 11.03/10.93 9.40/8.90 16.21/15.96 11.39/10.61\n",
      "2.0 54.28/51.95 9.39/9.36 11.53/10.75 14.21/14.04 13.81/12.63\n",
      "5.0 185.89/176.82 11.82/11.78 13.73/12.59 16.59/16.44 16.24/14.62\n",
      "4.3.3 Autoregressive video extension for longer sequences\n",
      "In Section 3.1 we proposed the reconstruction guidance method for conditional sampling from\n",
      "diffusion models, an improvement over the replacement method of [48]. In Table 6 we present\n",
      "results on generating longer videos using both techniques, and ﬁnd that our proposed method indeed\n",
      "improves over the replacement method in terms of perceptual quality scores.\n",
      "Figure 4 shows the samples of our reconstruction guidance method for conditional sampling compared\n",
      "to the replacement method (Section 3.1) for the purposes of generating long samples in a block-\n",
      "autoregressive manner (Section 4.3.3). The samples from the replacement method clearly show a\n",
      "lack of temporal coherence, since frames from different blocks throughout the generated videos\n",
      "appear to be uncorrelated samples (conditioned on c). The samples from the reconstruction guidance\n",
      "method, by contrast, are clearly temporally coherent over the course of the entire autoregressive\n",
      "generation process. Figure 2 additionally shows samples of using the reconstruction guidance method\n",
      "to simultaneously condition on low frequency, low resolution videos while autoregressively extending\n",
      "temporally at a high resolution.\n",
      "Table 6: Generating 64x64x64 videos using autoregressive extension of 16x64x64 models.\n",
      "Guidance weight Conditioning method FVD # FID-avg# IS-avg\" FID-ﬁrst# IS-ﬁrst\"\n",
      "2.0 reconstruction guidance 136.22/134.55 13.77/13.62 10.30/9.66 16.34/16.46 14.67/13.37\n",
      "replacement 451.45/436.16 25.95/25.52 7.00/6.75 16.33/16.46 14.67/13.34\n",
      "5.0 reconstruction guidance 133.92/133.04 13.59/13.58 10.31/9.65 16.28/16.53 15.09/13.72\n",
      "replacement 456.24/441.93 26.05/25.69 7.04/6.78 16.30/16.54 15.11/13.69\n",
      "8\n",
      "Figure 4: Comparing the replacement method (left) vs the reconstruction guidance method (right) for\n",
      "conditioning for block-autoregressive generation of 64 frames from a 16 frame model. Video frames\n",
      "are displayed over time from left to right; each row is an independent sample. The replacement\n",
      "method suffers from a lack of temporal coherence, unlike the reconstruction guidance method.\n",
      "5 Related work\n",
      "Prior work on video generation has usually employed other types of generative models, notably,\n",
      "autoregressive models, V AEs, GANs, and normalizing ﬂows [e.g. 3,4,32,30,14,59,62,57].\n",
      "Related work on model classes similar to diffusion models includes [ 25,24]. Concurrent work\n",
      "[63] proposes a diffusion-based approach to video generation that uses an image diffusion model to\n",
      "predict each individual frame within a RNN temporal autoregressive model. Our video diffusion\n",
      "model, by contrast, jointly models entire videos (blocks of frames) using a 3D video architecture\n",
      "with interleaved spatial and temporal attention, and we extend to long sequence lengths by ﬁlling in\n",
      "frames or autoregressive temporal extension.\n",
      "6 Conclusion\n",
      "We have introduced diffusion models for video modeling, thus bringing recent advances in generative\n",
      "modeling using diffusion models to the video domain. We have shown that with straightforward\n",
      "extensions of conventional U-Net architectures for 2D image modeling to 3D space-time, with\n",
      "factorized space-time attention blocks, one can learn effective generative models for video data using\n",
      "the standard formulation of the diffusion model. This includes unconditional models, text-conditioned\n",
      "models, and video prediction models.\n",
      "We have additionally demonstrated the beneﬁts of joint image-video training and classiﬁer-free\n",
      "guidance for video diffusion models on both video and image sample quality metrics, and we also\n",
      "introduced a new reconstruction-guided conditional sampling method that outperforms existing\n",
      "replacement or imputation methods for conditional sampling from unconditionally trained models.\n",
      "Our reconstruction guidance method can generate long sequences using either frame interpolation (or\n",
      "temporal super-resolution) or extrapolation in an auto-regressive fashion, and also can perform spatial\n",
      "super-resolution. We look forward to investigating this method in a wider variety of conditioning\n",
      "settings.\n",
      "Our goal with this work is to advance research on methods in generative modeling, and our methods\n",
      "have the potential to positively impact creative downstream applications. As with prior work in\n",
      "generative modeling, however, our methods have the potential for causing harmful impact and\n",
      "could enhance malicious or unethical uses of generative models, such as fake content generation,\n",
      "harassment, and misinformation spread, and thus we have decided not to release our models. Like\n",
      "all generative models, our models reﬂect the biases of their training datasets and thus may require\n",
      "curation to ensure fair results from sampling. In particular, our text-to-video models inherit the\n",
      "challenges faced by prior work on text-to-image models, and our future work will involve auditing for\n",
      "forms of social bias, similar to [ 6,7,50,12] for image-to-text and image labeling models. We see our\n",
      "work as only a starting point for further investigation on video diffusion models and investigation into\n",
      "9\n",
      "their societal implications, and we will aim to explore benchmark evaluations for social and cultural\n",
      "bias in the video generation setting and make the necessary research advances to address them.\n",
      "References\n",
      "[1]TensorFlow Datasets, a collection of ready-to-use datasets. https://www.tensorflow.org/\n",
      "datasets , 2022.\n",
      "[2]Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu ˇci´c, and Cordelia\n",
      "Schmid. ViViT: A video vision transformer. In Proceedings of the IEEE/CVF International\n",
      "Conference on Computer Vision , pages 6836–6846, 2021.\n",
      "[3]Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H Campbell, and Sergey Levine.\n",
      "Stochastic variational video prediction. arXiv preprint arXiv:1710.11252 , 2017.\n",
      "[4]Mohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj Nair, Sergey Levine, Chelsea Finn,\n",
      "and Dumitru Erhan. FitVid: Overﬁtting in pixel-level video prediction. arXiv preprint\n",
      "arXiv:2106.13195 , 2021.\n",
      "[5]Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for\n",
      "video understanding. arXiv preprint arXiv:2102.05095 , 2(3):4, 2021.\n",
      "[6]Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in\n",
      "commercial gender classiﬁcation. In Conference on Fairness, Accountability and Transparency,\n",
      "FAT 2018, 23-24 February 2018, New York, NY, USA , Proceedings of Machine Learning\n",
      "Research. PMLR, 2018.\n",
      "[7]Kaylee Burns, Lisa Hendricks, Trevor Darrell, and Anna Rohrbach. Women also snowboard:\n",
      "Overcoming bias in captioning models. In European Conference on Computer Vision (ECCV) ,\n",
      "2018.\n",
      "[8]Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the\n",
      "kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition , pages 6299–6308, 2017.\n",
      "[9]Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A\n",
      "short note about kinetics-600. arXiv preprint arXiv:1808.01340 , 2018.\n",
      "[10] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan.\n",
      "WaveGrad: Estimating gradients for waveform generation. International Conference on Learn-\n",
      "ing Representations , 2021.\n",
      "[11] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An improved\n",
      "autoregressive generative model. In International Conference on Machine Learning , pages\n",
      "863–871, 2018.\n",
      "[12] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social\n",
      "biases of text-to-image generative transformers. arxiv:2202.04053 , 2022.\n",
      "[13] Özgün Çiçek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger.\n",
      "3d u-net: learning dense volumetric segmentation from sparse annotation. In International\n",
      "conference on medical image computing and computer-assisted intervention , pages 424–432.\n",
      "Springer, 2016.\n",
      "[14] Aidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial video generation on complex\n",
      "datasets. arXiv preprint arXiv:1907.06571 , 2019.\n",
      "[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training\n",
      "of deep bidirectional transformers for language understanding. In Proceedings of the 2019\n",
      "Conference of the North American Chapter of the Association for Computational Linguistics:\n",
      "Human Language Technologies, NAACL-HLT , pages 4171–4186. Association for Computational\n",
      "Linguistics, 2019.\n",
      "10\n",
      "[16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis.\n",
      "Advances in Neural Information Processing Systems , 34, 2021.\n",
      "[17] Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-supervised visual planning\n",
      "with temporal skip connections. In CoRL , pages 344–356, 2017.\n",
      "[18] Cade Gordon and Natalie Parde. Latent neural differential equations for video generation. In\n",
      "NeurIPS 2020 Workshop on Pre-registration in Machine Learning , pages 73–86. PMLR, 2021.\n",
      "[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\n",
      "GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances\n",
      "in Neural Information Processing Systems , pages 6626–6637, 2017.\n",
      "[20] Jonathan Ho and Tim Salimans. Classiﬁer-free diffusion guidance. In NeurIPS 2021 Workshop\n",
      "on Deep Generative Models and Downstream Applications , 2021.\n",
      "[21] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in\n",
      "multidimensional transformers. arXiv preprint arXiv:1912.12180 , 2019.\n",
      "[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In\n",
      "Advances in Neural Information Processing Systems , pages 6840–6851, 2020.\n",
      "[23] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim\n",
      "Salimans. Cascaded diffusion models for high ﬁdelity image generation. arXiv preprint\n",
      "arXiv:2106.15282 , 2021.\n",
      "[24] Zahra Kadkhodaie and Eero Simoncelli. Stochastic solutions for linear inverse problems using\n",
      "the prior implicit in a denoiser. Advances in Neural Information Processing Systems , 34, 2021.\n",
      "[25] Zahra Kadkhodaie and Eero P Simoncelli. Solving linear inverse problems using the prior\n",
      "implicit in a denoiser. arXiv preprint arXiv:2007.13640 , 2020.\n",
      "[26] Emmanuel Kahembwe and Subramanian Ramamoorthy. Lower dimensional kernels for video\n",
      "discriminators. Neural Networks , 132:506–520, 2020.\n",
      "[27] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-\n",
      "narasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human\n",
      "action video dataset. arXiv preprint arXiv:1705.06950 , 2017.\n",
      "[28] Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models.\n",
      "arXiv preprint arXiv:2107.00630 , 2021.\n",
      "[29] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. DiffWave: A\n",
      "versatile diffusion model for audio synthesis. In 9th International Conference on Learning\n",
      "Representations, ICLR , 2021.\n",
      "[30] Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, Laurent\n",
      "Dinh, and Durk Kingma. VideoFlow: A ﬂow-based generative model for video. arXiv preprint\n",
      "arXiv:1903.01434 , 2019.\n",
      "[31] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. Ccvs: Context-aware controllable\n",
      "video synthesis. Advances in Neural Information Processing Systems , 34, 2021.\n",
      "[32] Alex X Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea Finn, and Sergey Levine.\n",
      "Stochastic adversarial video prediction. arXiv preprint arXiv:1804.01523 , 2018.\n",
      "[33] Pauline Luc, Aidan Clark, Sander Dieleman, Diego de Las Casas, Yotam Doron, Albin Cassirer,\n",
      "and Karen Simonyan. Transformation-based adversarial video prediction on large-scale data.\n",
      "arXiv preprint arXiv:2003.04035 , 2020.\n",
      "[34] Jacob Menick and Nal Kalchbrenner. Generating high ﬁdelity images with subscale pixel\n",
      "networks and multidimensional upscaling. In International Conference on Learning Represen-\n",
      "tations , 2019.\n",
      "11\n",
      "[35] Charlie Nash, João Carreira, Jacob Walker, Iain Barr, Andrew Jaegle, Mateusz Malinowski, and\n",
      "Peter Battaglia. Transframer: Arbitrary frame prediction with generative models. arXiv preprint\n",
      "arXiv:2203.09494 , 2022.\n",
      "[36] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\n",
      "Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing\n",
      "with text-guided diffusion models. arXiv preprint arXiv:2112.10741 , 2021.\n",
      "[37] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic\n",
      "models. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International\n",
      "Conference on Machine Learning, ICML , 2021.\n",
      "[38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for\n",
      "biomedical image segmentation. In International Conference on Medical Image Computing\n",
      "and Computer-Assisted Intervention , pages 234–241. Springer, 2015.\n",
      "[39] Chitwan Saharia, William Chan, Huiwen Chang, Chris A Lee, Jonathan Ho, Tim Salimans,\n",
      "David J Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. arXiv\n",
      "preprint arXiv:2111.05826 , 2021.\n",
      "[40] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad\n",
      "Norouzi. Image super-resolution via iterative reﬁnement. arXiv preprint arXiv:2104.07636 ,\n",
      "2021.\n",
      "[41] Masaki Saito, Shunta Saito, Masanori Koyama, and Sosuke Kobayashi. Train sparsely, generate\n",
      "densely: Memory-efﬁcient unsupervised training of high-resolution temporal GAN. Interna-\n",
      "tional Journal of Computer Vision , 128(10):2586–2606, 2020.\n",
      "[42] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models.\n",
      "InInternational Conference on Learning Representations , 2021.\n",
      "[43] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\n",
      "Improved techniques for training gans. In Advances in Neural Information Processing Systems ,\n",
      "pages 2234–2242, 2016.\n",
      "[44] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the\n",
      "PixelCNN with discretized logistic mixture likelihood and other modiﬁcations. In International\n",
      "Conference on Learning Representations , 2017.\n",
      "[45] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position repre-\n",
      "sentations. arXiv preprint arXiv:1803.02155 , 2018.\n",
      "[46] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\n",
      "vised learning using nonequilibrium thermodynamics. In International Conference on Machine\n",
      "Learning , pages 2256–2265, 2015.\n",
      "[47] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data\n",
      "distribution. In Advances in Neural Information Processing Systems , pages 11895–11907, 2019.\n",
      "[48] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon,\n",
      "and Ben Poole. Score-based generative modeling through stochastic differential equations.\n",
      "International Conference on Learning Representations , 2021.\n",
      "[49] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. A dataset of 101 human actions\n",
      "classes from videos in the wild. CRCV-TR-12-01 , 2012.\n",
      "[50] Ryan Steed and Aylin Caliskan. Image representations learned with unsupervised pre-training\n",
      "contain human-like biases. In Proceedings of the 2021 ACM Conference on Fairness, Account-\n",
      "ability, and Transparency , FAccT ’21, page 701–713. Association for Computing Machinery,\n",
      "2021.\n",
      "[51] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spa-\n",
      "tiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international\n",
      "conference on computer vision , pages 4489–4497, 2015.\n",
      "12\n",
      "[52] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing\n",
      "motion and content for video generation. In Proceedings of the IEEE conference on computer\n",
      "vision and pattern recognition , pages 1526–1535, 2018.\n",
      "[53] Belinda Tzen and Maxim Raginsky. Neural stochastic differential equations: Deep latent\n",
      "gaussian models in the diffusion limit. arXiv preprint arXiv:1905.09883 , 2019.\n",
      "[54] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski,\n",
      "and Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges.\n",
      "arXiv preprint arXiv:1812.01717 , 2018.\n",
      "[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
      "Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\n",
      "tion Processing Systems , pages 5998–6008, 2017.\n",
      "[56] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural\n",
      "Computation , 23(7):1661–1674, 2011.\n",
      "[57] Jacob Walker, Ali Razavi, and Aäron van den Oord. Predicting video with vqvae. arXiv preprint\n",
      "arXiv:2103.01950 , 2021.\n",
      "[58] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks.\n",
      "InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages\n",
      "7794–7803, 2018.\n",
      "[59] Dirk Weissenborn, Oscar Täckström, and Jakob Uszkoreit. Scaling autoregressive video models.\n",
      "InInternational Conference on Learning Representations , 2019.\n",
      "[60] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G Dimakis,\n",
      "and Peyman Milanfar. Deblurring via stochastic reﬁnement. arXiv preprint arXiv:2112.02475 ,\n",
      "2021.\n",
      "[61] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. NÜWA:\n",
      "Visual synthesis pre-training for neural visual world creation. arXiv preprint arXiv:2111.12417 ,\n",
      "2021.\n",
      "[62] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation\n",
      "using vq-vae and transformers. arXiv preprint arXiv:2104.10157 , 2021.\n",
      "[63] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Diffusion probabilistic modeling for\n",
      "video generation. arXiv preprint arXiv:2203.09481 , 2022.\n",
      "[64] Vladyslav Yushchenko, Nikita Araslanov, and Stefan Roth. Markov decision process for video\n",
      "generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision\n",
      "Workshops , pages 0–0, 2019.\n",
      "[65] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint\n",
      "arXiv:1605.07146 , 2016.\n",
      "13\n",
      "A Details and hyperparameters\n",
      "Figure 5: More samples accompanying Fig. 2.\n",
      "Here, we list the hyperparameters, training details, and compute resources used for each model.\n",
      "A.1 UCF101\n",
      "Base channels: 256 Optimizer: Adam ( \f1= 0:9;\f2= 0:99)\n",
      "Channel multipliers: 1, 2, 4, 8 Learning rate: 0.0003\n",
      "Blocks per resolution: 2 Batch size: 128\n",
      "Attention resolutions: 8, 16, 32 EMA: 0.9999\n",
      "Attention head dimension: 64 Dropout: 0.1\n",
      "Conditioning embedding dimension: 1024 Training hardware: 128 TPU-v4 chips\n",
      "Conditioning embedding MLP layers: 4 Training steps: 60000\n",
      "Diffusion noise schedule: cosine Joint training independent images per video: 8\n",
      "Noise schedule log SNR range: [\u000020;20] Sampling timesteps: 256\n",
      "= 0:1 resolution: 16x64x64 frameskip 1 Sampling log-variance interpolation: \n",
      "Weight decay: 0.0 Prediction target: \u000f\n",
      "A.2 BAIR Robot Pushing\n",
      "Base channels: 128 Optimizer: Adam ( \f1= 0:9;\f2= 0:999)\n",
      "Channel multipliers: 1, 2, 3, 4 Learning rate: 0.0002\n",
      "Blocks per resolution: 3 Batch size: 128\n",
      "Attention resolutions: 8, 16, 32 EMA: 0.999\n",
      "Attention head dimension: 64 Dropout: 0.1\n",
      "Conditioning embedding dimension: 1024 Training hardware: 128 TPU-v4 chips\n",
      "Conditioning embedding MLP layers: 2 Training steps: 660000\n",
      "Diffusion noise schedule: cosine Joint training independent images per video: 8\n",
      "Noise schedule log SNR range: [\u000020;20] Sampling timesteps: 256 (+256 Langevin cor.)\n",
      "= 0:0 resolution: 16x64x64 frameskip 1 Sampling log-variance interpolation: \n",
      "Weight decay: 0.01 Prediction target: v\n",
      "Reconstruction guidance weight: 50 Data augmentation: left-right ﬂips\n",
      "14\n",
      "A.3 Kinetics\n",
      "Base channels: 256 Optimizer: Adam ( \f1= 0:9;\f2= 0:99)\n",
      "Channel multipliers: 1, 2, 4, 8 Learning rate: 0.0002\n",
      "Blocks per resolution: 2 Batch size: 256\n",
      "Attention resolutions: 8, 16, 32 EMA: 0.9999\n",
      "Attention head dimension: 64 Dropout: 0.1\n",
      "Conditioning embedding dimension: 1024 Training hardware: 256 TPU-v4 chips\n",
      "Conditioning embedding MLP layers: 2 Training steps: 220,000\n",
      "Diffusion noise schedule: cosine Joint training independent images per video: 8\n",
      "Noise schedule log SNR range: [\u000020;20] Sampling timesteps: 128 (+128 Langevin cor.)\n",
      "= 0:0 resolution: 16x64x64 frameskip 1 Sampling log-variance interpolation: \n",
      "Weight decay: 0.0 Prediction target: v\n",
      "Reconstruction guidance weight: 9\n",
      "A.4 Text-to-video\n",
      "Small 16x64x64 model\n",
      "Base channels: 128 Optimizer: Adam ( \f1= 0:9;\f2= 0:99)\n",
      "Channel multipliers: 1, 2, 4, 8 Learning rate: 0.0003\n",
      "Blocks per resolution: 2 Batch size: 128\n",
      "Attention resolutions: 8, 16, 32 EMA: 0.9999\n",
      "Attention head dimension: 64 Dropout: 0.0\n",
      "Conditioning embedding dimension: 1024 Training hardware: 64 TPU-v4 chips\n",
      "Conditioning embedding MLP layers: 4 Training steps: 200000\n",
      "Diffusion noise schedule: cosine Joint training independent images per video: 0, 4, 8\n",
      "Noise schedule log SNR range: [\u000020;20] Sampling timesteps: 256\n",
      "= 0:3 resolution: 16x64x64 frameskip 1 Sampling log-variance interpolation: \n",
      "Weight decay: 0.0 Prediction target: \u000f\n",
      "Large 16x64x64 model\n",
      "Base channels: 256 Optimizer: Adam ( \f1= 0:9;\f2= 0:99)\n",
      "Channel multipliers: 1, 2, 4, 8 Learning rate: 0.0003\n",
      "Blocks per resolution: 2 Batch size: 128\n",
      "Attention resolutions: 8, 16, 32 EMA: 0.9999\n",
      "Attention head dimension: 64 Dropout: 0.0\n",
      "Conditioning embedding dimension: 1024 Training hardware: 128 TPU-v4 chips\n",
      "Conditioning embedding MLP layers: 4 Training steps: 700000\n",
      "Diffusion noise schedule: cosine Joint training independent images per video: 8\n",
      "Noise schedule log SNR range: [\u000020;20] Sampling timesteps: 256\n",
      "= 0:3 resolution: 16x64x64 frameskip 1,4 Sampling log-variance interpolation: \n",
      "Weight decay: 0.0 Prediction target: \u000f\n",
      "Large 9x128x128 model\n",
      "Base channels: 128 Optimizer: Adam ( \f1= 0:9;\f2= 0:99)\n",
      "Channel multipliers: 1, 2, 4, 8, 16 Learning rate: 0.0002\n",
      "Blocks per resolution: 2 Batch size: 128\n",
      "Attention resolutions: 8, 16, 32 EMA: 0.9999\n",
      "Attention head dimension: 128 Dropout: 0.0\n",
      "Conditioning embedding dimension: 1024 Training hardware: 128 TPU-v4 chips\n",
      "Conditioning embedding MLP layers: 4 Training steps: 800000\n",
      "Diffusion noise schedule: cosine Joint training independent images per video: 7\n",
      "Noise schedule log SNR range: [\u000020;20] Sampling timesteps: 256\n",
      "= 0:3 resolution: 9x128x128 frameskip 1 Sampling log-variance interpolation: \n",
      "Weight decay: 0.0 Prediction target: \u000f\n",
      "15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('hotseat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "913e0bdf535b9359a7637801bcbd63117994aa739fd9820024a6fcca35e62374"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
